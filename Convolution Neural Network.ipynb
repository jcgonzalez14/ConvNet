{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolution Neural Network.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ANt5Z6ALSKzo"
      },
      "source": [
        "# CNN Facial Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FhtKpOyKSKzt"
      },
      "source": [
        "## 1. Introduction\n",
        "Hello! My selfgiven name is the Stats Whisper and I'm back with another Data Science topic that is one of the hottest things in research now: Deep Learning. Even if you lend a very small ear to what is going on in the scientific community, you have probably heard of Deep Learning. It is the backbone of some of the coolest things today like: self-driving cars, facial-recongition, virtual digital assistants and [accurately estimating how a human on the other side of a wall is standing/sitting/walking just from perturbations in Wifi signals caused by that human](http://rfpose.csail.mit.edu/) (don't know what good that is for but hey).\n",
        "\n",
        "\n",
        "Believe or not, neural networks have been around for a while but the slashed costs of computing power paired with the increased computing capacity are the impetus behind the eponential growth of the field. \n",
        "\n",
        "With the expantion of the field grew different kinds of neural networks for different purposes. \n",
        "Today, we will be focusing on a particular type of neural network called a convolution neural network (CNN for short). Research has found these models to be excellent when it comes to image recognition. \n",
        "\n",
        "Okay, so before we dive into what exactly a convulution neural network is let us strip it down to its essence, just a plain and simple vanilla neural network. Like the depths of the universe, you can go as far, wide and deep into this topic and you'll probably never get to the bottom of it because of the presence of a very active area of research that is pushing the field even further. \n",
        "\n",
        "So what exactly is a neural network? Well, it's actually pretty simple: a function. A function that can be very complex but a function nevertheless. \n",
        "\n",
        "At its most basic core, a neural network has 3 parts:\n",
        " - input layer\n",
        " - hidden layer\n",
        " - output layer\n",
        "\n",
        "As the name suggests, the input layer is where the data is feed into the network. The mysterious hidden layer is where all the action is happening and output layer is the result you wish to acquire. \n",
        "\n",
        "In relation to a function, take the ubiquitous function of simple line found in every Algebra class: y = mx+b. The x in this case is the input layer. The m and b are hidden layer and the y is the output layer. \n",
        "\n",
        "Easy right?\n",
        "\n",
        "A convolution neural network does the exact same thing by using an image as input, crunches some numbers then does a prediction as an output.\n",
        "\n",
        "There is a great amount of content out there that goes into good detail and further explains how the mechanics of a convolution neural network work. It can get very mathematical and technical really quick so you can easily lose an audience. But if you are really interested to go further, I found [this video](https://www.youtube.com/watch?v=aircAruvnKk) to be a lifesaver for my Big Data class while I was learning neural networks in grad school.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JYK8DjBBSKzv"
      },
      "source": [
        "## 2. Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7KvhLsW6SKzw"
      },
      "source": [
        "If you ever wondered how in the world your new Iphone has the capacity to determine whether or not it's your face or someone else's face with amazing accuracy to unlock your phone using Face ID, well you have just witnessed the power of neural networks first hand. Apple trained the neural networks using billions of images then took the result and installed it into your iphone. Pretty cool, huh?\n",
        "\n",
        "Since neural networks are great at determining who's face that image belongs to, what if we could weild this technology.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al-5EirTF1-K",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Loading the Data\n",
        "\n",
        "Okay so first let's load the data. We'll be using Google's Colab product to leverage the powerfull GPU available, slashing runtimes instead waiting forever for it to run locally on my MacBook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL5s_wKZF1-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading google drive where data is stored to run it on Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#importing the necesary libraries and modules to compile data for ingest by CNN.\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import TensorBoard\n",
        "K.set_image_dim_ordering('th')\n",
        "from time import time\n",
        "batch_size = 64\n",
        "\n",
        "train_datagen = ImageDataGenerator(data_format=\"channels_last\") \n",
        "\n",
        "test_datagen = ImageDataGenerator(data_format=\"channels_last\")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/gdrive/My Drive/data/train',  # this is the target directory on google drive\n",
        "        target_size=(197, 197),  # all images will be resized to 197x197\n",
        "        batch_size=batch_size,\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical') \n",
        "\n",
        "# this is a similar generator, for validation data\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        '/content/gdrive/My Drive/data/validation',\n",
        "        target_size=(197, 197),\n",
        "        batch_size=batch_size,\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical')\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=\"tensorBoard/\".format(time()), write_images=True, write_graph=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT6CDsJ_F1-R",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 CNN Transfer Learning\n",
        "\n",
        "Instead of creating a CNN from scratch and having to use valuable time and resources training/testing different models, what we can do instead is use a model that has already been trained then slightly customize it to fit our objectives. The formal term for this technique is called \"transfer learning\". \n",
        "\n",
        "Transfer learning is great because you can take neural networks and their respective weights painstakingly developed by the pros then reuse those models for other purposes. \n",
        "\n",
        "While TensorFlow is the gold standard for neural networks I perfer to use Keras because of the way it simplyfies long cumbersome TensorFlow code into a few simple lines of code and, more importantly, Keras comes pre-trained models available for execute transfer learning. \n",
        "\n",
        "Today, we'll be using a CNN developed by researchers at Cornell University called [MobileNetV2](https://arxiv.org/abs/1801.04381). While most of the available models have compariable accuracy rates, this model is \"lightweight\" with it's fewer total parameters in the model and thus faster to train. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byi6i67RF1-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing the relevant libraries and modules for CNN\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n",
        "from keras import backend as K\n",
        "import keras\n",
        "\n",
        "input_tensor = Input(shape=(197, 197, 3))\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model = MobileNetV2(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "# since we have 3 classes, we need a final layer that predicts 3 classes. \n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "custom_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# compile the model\n",
        "custom_model.compile(optimizer=keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "custom_model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=10,\n",
        "        epochs=10,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfjBWVw-F1-c",
        "colab_type": "text"
      },
      "source": [
        "You'll notice that the accuracy of the model rapidly increases after each iteration. That's because the model is leveraging the features it previously learned and putting them to use to make sense of the images it is being feed. You are witnessing the power of transfer learning in action.\n",
        "\n",
        "So now with a fully trained model at hand, we can use machine learning to help us with in our quest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EMAC36xKT_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "img_path = 'elephant.jpg' # provide my image. \n",
        "img = image.load_img(img_path, target_size=(197, 197))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = custom_model.predict(x)\n",
        "# decode the results into a list of tuples (class, description, probability)\n",
        "# (one such list for each sample in the batch)\n",
        "print('Predicted:', decode_predictions(preds, top=3)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}